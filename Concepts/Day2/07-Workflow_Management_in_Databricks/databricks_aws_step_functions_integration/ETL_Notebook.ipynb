{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ETL Notebook for Databricks\n", "df = spark.read.csv('s3://your-bucket-name/raw-data.csv', header=True, inferSchema=True)\n", "df_cleaned = df.filter(df['column_name'].isNotNull())\n", "df_cleaned.write.parquet('s3://your-bucket-name/processed-data/')"]}], "metadata": {}, "nbformat": 4, "nbformat_minor": 5}