{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0efb88a9-9d74-406f-9521-e6e3a3bdc368",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Using Databricks Pipelines for Data Workflows\n",
    "\n",
    "## 1. Introduction to Databricks Pipelines\n",
    "\n",
    "**Databricks Pipelines** provide an easy-to-use, scalable way to create ETL workflows using Delta Live Tables. These pipelines automate data ingestion, transformation, and storage with a structured approach to manage workflows efficiently.\n",
    "\n",
    "### Key Benefits:\n",
    "- **Automated Workflow Management**: Reduces manual ETL tasks by automating data transformations and quality checks.\n",
    "- **Data Quality Assurance**: Built-in tools enforce data quality with expectations and automatic schema evolution.\n",
    "- **Reliability with ACID Transactions**: Delta Lake offers ACID transactions for reliable data processing.\n",
    "- **Real-Time Data Processing**: DLT supports both batch and real-time data workflows.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Setting Up a Delta Live Tables Pipeline\n",
    "\n",
    "To use Databricks Pipelines, set up a Delta Live Tables pipeline within your Databricks environment.\n",
    "\n",
    "### Step-by-Step Setup\n",
    "\n",
    "1. **Navigate to Delta Live Tables**:\n",
    "   - In your Databricks workspace, go to the **Workflows** tab, then select **Delta Live Tables**.\n",
    "\n",
    "2. **Create a New Pipeline**:\n",
    "   - Click on **Create Pipeline**.\n",
    "   - Name the pipeline (e.g., `RetailDataWorkflow`).\n",
    "   - **Mode**: Choose **Triggered** for batch processing or **Continuous** for streaming (real-time) processing.\n",
    "\n",
    "3. **Define Source and Target Locations**:\n",
    "   - Specify the path to your source data (e.g., an S3 or ADLS path).\n",
    "   - Define a storage location for the DLT pipeline, such as `dbfs:/pipelines/retail_data_workflow/`.\n",
    "\n",
    "4. **Configure Advanced Settings** (Optional):\n",
    "   - Enable **Auto Schema Evolution** to adapt to changes in the source schema.\n",
    "   - Configure **Data Quality Expectations** to enforce rules on specific columns.\n",
    "\n",
    "### Example Pipeline Configuration\n",
    "\n",
    "```plaintext\n",
    "Pipeline Name: RetailDataWorkflow\n",
    "Pipeline Mode: Continuous\n",
    "Notebook Library: /Workspace/DeltaLiveTables/RetailDataPipeline\n",
    "Storage Location: dbfs:/pipelines/retail_data_workflow/\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Building a Data Workflow with Delta Live Tables\n",
    "\n",
    "The core of a Databricks Pipeline is the Delta Live Tables notebook, where tables and views are defined with data transformations and quality checks.\n",
    "\n",
    "### Step 1: Data Ingestion\n",
    "\n",
    "Define the ingestion layer with Delta Live Tables to load data from the source into a raw table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3ae5718-9079-4664-8bd6-4b4315f8ff0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define a schema for raw retail data\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"invoice_no\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"unit_price\", IntegerType(), True),\n",
    "    StructField(\"invoice_date\", TimestampType(), True),\n",
    "    StructField(\"country\", StringType(), True)\n",
    "])\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"raw_retail_data\",\n",
    "    comment=\"Ingest raw retail data from S3\"\n",
    ")\n",
    "def raw_retail_data():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .schema(schema)\n",
    "        .load(\"/mnt/s3dataread/retail-data/by-day/test-auto-loaded/\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4c6b55b-c88f-4331-9802-284b313e078c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 2: Data Transformation\n",
    "\n",
    "Create transformation layers to clean and standardize the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de068dd6-8aec-4b75-911f-5bef756488c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"cleaned_retail_data\",\n",
    "    comment=\"Cleaned and standardized retail data\"\n",
    ")\n",
    "def cleaned_retail_data():\n",
    "    return (\n",
    "        dlt.read(\"raw_retail_data\")\n",
    "        .filter(col(\"quantity\").isNotNull() & col(\"unit_price\").isNotNull())\n",
    "        .withColumn(\"total_value\", col(\"quantity\") * col(\"unit_price\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3136e2e-360f-419b-8cb9-1e928ab05644",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 3: Data Aggregation and Enrichment\n",
    "\n",
    "Aggregate data to create a summary table for analytical queries, such as total sales per customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09ca353a-2e61-41f0-af99-a59a295a9a9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"customer_sales_summary\",\n",
    "    comment=\"Aggregated sales data by customer\"\n",
    ")\n",
    "def customer_sales_summary():\n",
    "    return (\n",
    "        dlt.read(\"cleaned_retail_data\")\n",
    "        .groupBy(\"customer_id\")\n",
    "        .agg({\"total_value\": \"sum\"})\n",
    "        .withColumnRenamed(\"sum(total_value)\", \"total_sales\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "036e029d-63d4-4130-b322-0a0797a5afc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 4: Data Quality Checks\n",
    "\n",
    "Use **expectations** to enforce data quality rules. If data fails these expectations, it can be logged or discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d4f0e6f-e499-4db7-b698-51171ceb8098",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"validated_retail_data\",\n",
    "    comment=\"Data with quality checks on quantity and unit price\"\n",
    ")\n",
    "@dlt.expect(\"positive_quantity\", \"quantity > 0\")\n",
    "@dlt.expect(\"valid_price\", \"unit_price > 0\")\n",
    "def validated_retail_data():\n",
    "    return dlt.read(\"cleaned_retail_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08697dba-0c76-45de-b8df-3b0351163211",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this example:\n",
    "- `@dlt.expect`: Specifies data quality expectations. If rows violate these rules, they are either logged or dropped based on configuration.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Scheduling and Automation\n",
    "\n",
    "To automate this pipeline, configure it to run on a defined schedule.\n",
    "\n",
    "### Step-by-Step Scheduling Setup\n",
    "\n",
    "1. **Go to the Pipeline Settings**: In the Delta Live Tables UI, open the pipeline you created.\n",
    "2. **Select Schedule**:\n",
    "   - Configure the pipeline to run at intervals, such as daily or hourly, depending on your data update frequency.\n",
    "3. **Choose Mode**:\n",
    "   - **Triggered Mode**: Executes the pipeline based on a schedule or manually.\n",
    "   - **Continuous Mode**: Continuously processes incoming data in real-time.\n",
    "\n",
    "With a configured schedule, Databricks will automatically trigger the pipeline based on the set interval.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df5a2c73-e86f-471b-95d1-60edbd5c34d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 7. Summary\n",
    "\n",
    "This guide covered **Using Databricks Pipelines for Data Workflows** to create, manage, and optimize ETL processes:\n",
    "1. **Setting Up a Delta Live Tables Pipeline**: Configured a new pipeline and defined ingestion settings.\n",
    "2. **Building the Data Workflow**: Ingested data from source, transformed it, performed aggregation, and applied quality checks.\n",
    "3. **Scheduling and Automation**: Configured pipeline"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "14-Using-Databricks-pipelines-for-data-workflows",
   "widgets": {}
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
